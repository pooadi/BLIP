{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2F6pHle6S1V_"},"outputs":[],"source":["# code to mount my drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIcSo_9ZTPwz"},"outputs":[],"source":["# install the necessary requirements\n","\n","!pip install -r '/content/drive/My Drive/My_Software_Projects/BLIP/requirements.txt'"]},{"cell_type":"markdown","metadata":{"id":"fKZQFsPnUuNy"},"source":["**This section is for running an inference instant of BLIP**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OcUm83MxY-XB"},"outputs":[],"source":["%cd /content/drive/MyDrive/My_Software_Projects/BLIP\n","!pip install -Uqq ipdb\n","import ipdb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOtJbX4HiZTS"},"outputs":[],"source":["%pdb on"]},{"cell_type":"markdown","metadata":{"id":"CWeYTFBAprLc"},"source":["**Code to load images and preprocess them**"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5421,"status":"ok","timestamp":1687500405468,"user":{"displayName":"Aditya Poonja","userId":"00085961209034521168"},"user_tz":-330},"id":"fEQ9Ez71UfO9"},"outputs":[],"source":["from PIL import Image\n","import requests\n","import torch\n","from torchvision import transforms\n","from torchvision.transforms.functional import InterpolationMode\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def loadImage(imageSize, device, rawImage):\n","\n","    # imgUrl = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n","    # rawImage = Image.open(requests.get(imgUrl, stream=True).raw).convert('RGB')\n","    # imgUrl = '/content/drive/My Drive/My_Software_Projects/Input_Frames/time0_frame1.jpg'\n","    # rawImage = Image.open(imgUrl).convert('RGB')\n","    # rawImage = cv2.imread(imgUrl)\n","    rawImage = rawImage\n","    rawImage = cv2.cvtColor(rawImage, cv2.COLOR_BGR2RGB)\n","    rawImage = Image.fromarray(rawImage)\n","\n","\n","    w,h = rawImage.size\n","    display(rawImage.resize((w//5,h//5)))\n","    #cv2.imshow('image', raw_image)\n","\n","    transform = transforms.Compose([\n","        transforms.Resize((imageSize,imageSize),interpolation=InterpolationMode.BICUBIC),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n","        ])\n","    image = transform(rawImage).unsqueeze(0).to(device)\n","\n","    return image"]},{"cell_type":"markdown","metadata":{"id":"OTvllp8fqNXe"},"source":["**Make directories utli function**"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":406,"status":"ok","timestamp":1687500410706,"user":{"displayName":"Aditya Poonja","userId":"00085961209034521168"},"user_tz":-330},"id":"g5oTHxmnqNFR"},"outputs":[],"source":["import os\n","\n","def makeDirectory(path):\n","\n","    try:\n","\n","        os.makedirs(path, exist_ok=True)\n","        print(\"Directory '%s' created successfully\" % path)\n","\n","    except OSError as error:\n","\n","        print(\"Directory already exist\")\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"lFQpveR-Fd8w"},"source":["**Code to run an inference on the BLIP image captioning network from a video file**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBUaGspLVKPU"},"outputs":[],"source":["from models.blip import blip_decoder\n","from PIL import Image\n","import requests\n","import torch\n","from torchvision import transforms\n","from torchvision.transforms.functional import InterpolationMode\n","import cv2\n","import time\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","currentTime = time.strftime(\"%H_%M_%S\")\n","\n","#path to input Video File\n","pathToInputVideoFile = '/content/drive/My Drive/My_Software_Projects/Input_Video/InputVideo2.mp4'\n","\n","#path to the output Folder\n","pathToOutputFramesFolder = '/content/drive/My Drive/My_Software_Projects/Output/Output_Caption_' + str(currentTime) + '/'\n","makeDirectory(pathToOutputFramesFolder)\n","\n","# path to the output text File\n","pathToOutputTxtFile = '/content/drive/My Drive/My_Software_Projects/Output_File/CaptionTextFile_' + str(currentTime) + '.txt'\n","\n","# set image size\n","imageSize = 384\n","\n","#Loading the model file\n","modelUrl = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n","\n","\n","\n","model = blip_decoder(pretrained=modelUrl, image_size=imageSize, vit='base')\n","model.eval()\n","model = model.to(device)\n","print(\"Model Successfully Loaded.\")\n","\n","print(\"Loading Input Video File.....\")\n","inputVideo = cv2.VideoCapture(pathToInputVideoFile)\n","print(\"Successfully Loaded input File\")\n","\n","\n","# Calculate the Frames per second (FPS)\n","print(\"Calculating Frames Per Second...\")\n","fps = round(inputVideo.get(cv2.CAP_PROP_FPS))\n","print('Fps = ' + str(fps))\n","\n","frameNumber = 0\n","timeStamp = 0\n","\n","print('Processing Frames...')\n","\n","\n","while True:\n","  # Processing Frames\n","  success, imageFrame = inputVideo.read()\n","\n","  if success:\n","    # increase the frame by 1\n","    frameNumber += 1\n","\n","    image = loadImage(imageSize=imageSize, device=device, rawImage=imageFrame)\n","\n","    with torch.no_grad():\n","\n","      # ipdb.set_trace(context=6)\n","      # beam search\n","      caption = model.generate(image, sample=False, num_beams=3, max_length=20, min_length=5)\n","\n","      # nucleus sampling\n","      #caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5)\n","\n","      print('caption: '+caption[0])\n","\n","      # outputFrameFilePath = pathToOutputFramesFolder + 'time' + str(timeStamp) + '_' + 'frame' + str(frameNumber) + '.jpg'\n","\n","      outputFrameFilePath = pathToOutputFramesFolder + str(caption[0]) + '.jpg'\n","\n","      # write the frame\n","      cv2.imwrite(outputFrameFilePath, imageFrame)\n","\n","      # write the captions to an output file\n","      writingText = \"TimeStamp = \" + str(timeStamp) +  \" Frame = \" + str(frameNumber) + \" caption : \" + str(caption[0])\n","\n","      with open(pathToOutputTxtFile, 'a') as testwritefile:\n","        testwritefile.write(writingText + '\\n')\n","\n","      print('Time = ' + str(timeStamp) + ' secs Frame = ' + str(frameNumber) + ' saved successfully')\n","\n","  else:\n","\n","    break\n","\n","  # every 24 Frames increase the time by 1 and reset the frames to 0\n","  if frameNumber == fps:\n","\n","    timeStamp += 1\n","    frameNumber = 0\n","\n","print(\"Frame successfully Processed.\")\n","inputVideo.release()"]},{"cell_type":"markdown","metadata":{"id":"yOpdsUhvBXZm"},"source":["**Inference code for VQA (Visual Question Answering)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SCpMnzRHY8ta"},"outputs":[],"source":["from models.blip_vqa import blip_vqa\n","from models.blip import blip_decoder\n","from PIL import Image\n","import requests\n","import torch\n","from torchvision import transforms\n","from torchvision.transforms.functional import InterpolationMode\n","import cv2\n","import time\n","\n","# question to be Asked (Text Prompt)\n","question = 'What is the facial expression of the person?'\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","currentTime = time.strftime(\"%H_%M_%S\")\n","\n","#path to input Video File\n","pathToInputVideoFile = '/content/drive/My Drive/My_Software_Projects/Input_Video/InputVideo2.mp4'\n","\n","#path to the output Folder\n","pathToOutputFramesFolder = '/content/drive/My Drive/My_Software_Projects/Output/Output_VQA_' + str(currentTime) + '/'\n","makeDirectory(pathToOutputFramesFolder)\n","\n","# path to the output text File\n","pathToOutputTxtFile = '/content/drive/My Drive/My_Software_Projects/Output_File/CaptionTextFile_' + str(currentTime) + '.txt'\n","\n","imageSize = 480\n","# image = load_image(image_size=image_size, device=device)\n","\n","modelUrl = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n","\n","print(\"Loading the Pre-Trained Model...\")\n","\n","model = blip_vqa(pretrained=modelUrl, image_size=imageSize, vit='base')\n","model.eval()\n","model = model.to(device)\n","print(\"Model Successfully Loaded.\")\n","\n","print(\"Loading Input Video File.....\")\n","inputVideo = cv2.VideoCapture(pathToInputVideoFile)\n","print(\"Successfully Loaded input File\")\n","\n","\n","# Calculate the Frames per second (FPS)\n","print(\"Calculating Frames Per Second...\")\n","fps = round(inputVideo.get(cv2.CAP_PROP_FPS))\n","print('Fps = ' + str(fps))\n","\n","frameNumber = 0\n","timeStamp = 0\n","\n","print('Processing Frames...')\n","\n","\n","while True:\n","  # Processing Frames\n","  success, imageFrame = inputVideo.read()\n","\n","  if success:\n","    # increase the frame by 1\n","    frameNumber += 1\n","\n","    image = loadImage(imageSize=imageSize, device=device, rawImage=imageFrame)\n","\n","    with torch.no_grad():\n","\n","      answer = model(image, question, train=False, inference='generate')\n","      print('answer: '+answer[0])\n","\n","\n","      outputFrameFilePath = pathToOutputFramesFolder + str(answer[0]) + str(timeStamp) + str(frameNumber) + '.jpg'\n","\n","      # write the frame\n","      cv2.imwrite(outputFrameFilePath, imageFrame)\n","\n","      # write the captions\n","      writingText = \"TimeStamp = \" + str(timeStamp) +  \" Frame = \" + str(frameNumber) + \" Emotion : \" + str(answer[0])\n","      # utputTextFile.write(writingText)\n","\n","      with open(pathToOutputTxtFile, 'a') as testwritefile:\n","        testwritefile.write(writingText + '\\n')\n","\n","      # outputTextFile.write('\\n')\n","\n","      print('Time = ' + str(timeStamp) + ' secs Frame = ' + str(frameNumber) + ' saved successfully')\n","\n","  else:\n","\n","    break\n","\n","  # every 24 Frames increase the time by 1 and reset the frames to 0\n","  if frameNumber == fps:\n","\n","    timeStamp += 1\n","    frameNumber = 0\n","\n","print(\"Frame successfully Processed.\")\n","inputVideo.release()\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyNbu0rX4J/j3HaCnGmy8A3U"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}