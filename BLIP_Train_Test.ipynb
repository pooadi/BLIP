{"cells":[{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","###***Initialisation cells (Run these cells to download the necessary libraries and mount the drive)***\n","\n","\n","---\n","\n"],"metadata":{"id":"RI9etoGooW7c"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2F6pHle6S1V_"},"outputs":[],"source":["# code to mount my drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIcSo_9ZTPwz"},"outputs":[],"source":["# install the necessary requirements\n","\n","!pip install -r '/content/drive/My Drive/My_Software_Projects/BLIP/requirements.txt'\n","!pip install SpeechRecognition\n","!pip install moviepy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OcUm83MxY-XB"},"outputs":[],"source":["%cd /content/drive/MyDrive/My_Software_Projects/BLIP\n","!pip install -Uqq ipdb\n","import ipdb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOtJbX4HiZTS"},"outputs":[],"source":["%pdb on"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","###***Util codes that need to be run before executing the codes in the main section***\n","\n","\n","---\n","\n"],"metadata":{"id":"Wto2FAhIme8T"}},{"cell_type":"markdown","metadata":{"id":"CWeYTFBAprLc"},"source":["#### Code to load images and preprocess them"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"fEQ9Ez71UfO9","executionInfo":{"status":"ok","timestamp":1688685448234,"user_tz":-330,"elapsed":7185,"user":{"displayName":"Aditya Poonja","userId":"00085961209034521168"}}},"outputs":[],"source":["from PIL import Image\n","import requests\n","import torch\n","from torchvision import transforms\n","from torchvision.transforms.functional import InterpolationMode\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def loadImage(imageSize, device, rawImage):\n","\n","    # imgUrl = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n","    # rawImage = Image.open(requests.get(imgUrl, stream=True).raw).convert('RGB')\n","    # imgUrl = '/content/drive/My Drive/My_Software_Projects/Input_Frames/time0_frame1.jpg'\n","    # rawImage = Image.open(imgUrl).convert('RGB')\n","    # rawImage = cv2.imread(imgUrl)\n","    rawImage = rawImage\n","    rawImage = cv2.cvtColor(rawImage, cv2.COLOR_BGR2RGB)\n","    rawImage = Image.fromarray(rawImage)\n","\n","\n","    w,h = rawImage.size\n","    display(rawImage.resize((w//5,h//5)))\n","    #cv2.imshow('image', raw_image)\n","\n","    transform = transforms.Compose([\n","        transforms.Resize((imageSize,imageSize),interpolation=InterpolationMode.BICUBIC),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n","        ])\n","    image = transform(rawImage).unsqueeze(0).to(device)\n","\n","    return image"]},{"cell_type":"markdown","metadata":{"id":"OTvllp8fqNXe"},"source":["#### Make directories utli function\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"g5oTHxmnqNFR","executionInfo":{"status":"ok","timestamp":1688685459361,"user_tz":-330,"elapsed":2,"user":{"displayName":"Aditya Poonja","userId":"00085961209034521168"}}},"outputs":[],"source":["import os\n","\n","def makeDirectory(path):\n","\n","    try:\n","\n","        os.makedirs(path, exist_ok=True)\n","        print(\"Directory '%s' created successfully\" % path)\n","\n","    except OSError as error:\n","\n","        print(\"Directory already exist\")\n","        pass"]},{"cell_type":"markdown","source":["#### Util function to write CSV File"],"metadata":{"id":"R6GFEUrEyYbG"}},{"cell_type":"code","source":["import csv\n","\n","def csvWriteRow(pathToCSVFile, rowData):\n","\n","  \"\"\"\n","  Util function to write a CSV File\n","\n","  pathToCSVFile: path to the CSV File\n","  rowData: Array data to be written\n","\n","  \"\"\"\n","\n","  # write the csv File\n","  with open(pathToCSVFile, 'a', newline='') as testWriteCSV:\n","\n","    csvWriter = csv.writer(testWriteCSV)\n","    csvWriter.writerow(rowData)"],"metadata":{"id":"PiiIx0j1yXtG","executionInfo":{"status":"ok","timestamp":1688685489771,"user_tz":-330,"elapsed":405,"user":{"displayName":"Aditya Poonja","userId":"00085961209034521168"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#### Function to get the audio to Text from a video file"],"metadata":{"id":"7NrcGW5m5M4L"}},{"cell_type":"code","source":["import speech_recognition as sr\n","import moviepy.editor as mp\n","from moviepy.editor import VideoFileClip\n","from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n","\n","\n","def videoToText(pathToVideo):\n","  \"\"\"\n","  Util Function that converts video to text using the Google Speech to text API\n","\n","  :param pathToVideo : Path to the video file\n","  :return: List containing the audiototext per second\n","  \"\"\"\n","\n","  print (\"Converting speech to Text from Video..\")\n","  clip = VideoFileClip(pathToVideo)\n","\n","  numSecondsVideo = int(clip.duration)\n","  print(\"The video is {} seconds\".format(numSecondsVideo))\n","  lenList = list(range(0, numSecondsVideo + 1, 1))\n","\n","  resultDict = {}\n","  for i in range(len(lenList) - 1):\n","\n","    print(\"time = \" + str(i))\n","\n","    # clipping video into chunks of small sections to run through the speech APi\n","    ffmpeg_extract_subclip(pathToVideo, lenList[i] - 2 * (lenList[i] != 0), lenList[i + 1],\n","                           targetname=\"/content/drive/My Drive/My_Software_Projects/Intermediate_Folder/cut{}.mp4\".format(i + 1))\n","\n","    clip = mp.VideoFileClip(r\"/content/drive/My Drive/My_Software_Projects/Intermediate_Folder/cut{}.mp4\".format(i + 1))\n","\n","    # separating the audio from video\n","    clip.audio.write_audiofile(r\"/content/drive/My Drive/My_Software_Projects/Intermediate_Folder/converted{}.wav\".format(i + 1))\n","\n","    r = sr.Recognizer()\n","    audio = sr.AudioFile(\"/content/drive/My Drive/My_Software_Projects/Intermediate_Folder/converted{}.wav\".format(i + 1))\n","\n","    with audio as source:\n","      r.adjust_for_ambient_noise(source)\n","      audioFile = r.record(source)\n","\n","    try:\n","\n","      # feeding the sudio to the google speech to text API\n","      result = r.recognize_google(audioFile)\n","\n","    except sr.exceptions.UnknownValueError:\n","\n","      # store as no exception if the audio has not enough data to convert to text\n","      result = 'No transcript'\n","    resultDict['chunk{}'.format(i + 1)] = result\n","\n","  listText = [resultDict['chunk{}'.format(i + 1)] for i in range(len(resultDict))]\n","\n","  return listText\n","\n"],"metadata":{"id":"ivjVqRQb5LgL","executionInfo":{"status":"ok","timestamp":1688686203256,"user_tz":-330,"elapsed":433,"user":{"displayName":"Aditya Poonja","userId":"00085961209034521168"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","### ***Main code Section to run inference of BLIP on various modes and also dataset generation. (All the code cells above need to executed before the execution of the cells below)***\n","\n","\n","---\n"],"metadata":{"id":"HQzegzcRl4Mk"}},{"cell_type":"markdown","metadata":{"id":"lFQpveR-Fd8w"},"source":["#### Code to run an inference on the BLIP image captioning network from a video file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBUaGspLVKPU"},"outputs":[],"source":["from models.blip import blip_decoder\n","from PIL import Image\n","import requests\n","import torch\n","from torchvision import transforms\n","from torchvision.transforms.functional import InterpolationMode\n","import cv2\n","import time\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","currentTime = time.strftime(\"%H_%M_%S\")\n","\n","#path to input Video File\n","pathToInputVideoFile = '/content/drive/My Drive/My_Software_Projects/Input_Video/InputVideo2.mp4'\n","\n","#path to the output Folder\n","pathToOutputFramesFolder = '/content/drive/My Drive/My_Software_Projects/Output/Output_Caption_' + str(currentTime) + '/'\n","makeDirectory(pathToOutputFramesFolder)\n","\n","# path to the output text File\n","pathToOutputTxtFile = '/content/drive/My Drive/My_Software_Projects/Output_File/CaptionTextFile_' + str(currentTime) + '.txt'\n","\n","# set image size\n","imageSize = 384\n","\n","#Loading the model file\n","modelUrl = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n","\n","\n","\n","model = blip_decoder(pretrained=modelUrl, image_size=imageSize, vit='base')\n","model.eval()\n","model = model.to(device)\n","print(\"Model Successfully Loaded.\")\n","\n","print(\"Loading Input Video File.....\")\n","inputVideo = cv2.VideoCapture(pathToInputVideoFile)\n","print(\"Successfully Loaded input File\")\n","\n","\n","# Calculate the Frames per second (FPS)\n","print(\"Calculating Frames Per Second...\")\n","fps = round(inputVideo.get(cv2.CAP_PROP_FPS))\n","print('Fps = ' + str(fps))\n","\n","frameNumber = 0\n","timeStamp = 0\n","\n","print('Processing Frames...')\n","\n","\n","while True:\n","  # Processing Frames\n","  success, imageFrame = inputVideo.read()\n","\n","  if success:\n","    # increase the frame by 1\n","    frameNumber += 1\n","\n","    image = loadImage(imageSize=imageSize, device=device, rawImage=imageFrame)\n","\n","    with torch.no_grad():\n","\n","      # ipdb.set_trace(context=6)\n","      # beam search\n","      caption = model.generate(image, sample=False, num_beams=3, max_length=20, min_length=5)\n","\n","      # nucleus sampling\n","      #caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5)\n","\n","      print('caption: '+caption[0])\n","\n","      # outputFrameFilePath = pathToOutputFramesFolder + 'time' + str(timeStamp) + '_' + 'frame' + str(frameNumber) + '.jpg'\n","\n","      outputFrameFilePath = pathToOutputFramesFolder + str(caption[0]) + '.jpg'\n","\n","      # write the frame\n","      cv2.imwrite(outputFrameFilePath, imageFrame)\n","\n","      # write the captions to an output file\n","      writingText = \"TimeStamp = \" + str(timeStamp) +  \" Frame = \" + str(frameNumber) + \" caption : \" + str(caption[0])\n","\n","      with open(pathToOutputTxtFile, 'a') as testwritefile:\n","        testwritefile.write(writingText + '\\n')\n","\n","      print('Time = ' + str(timeStamp) + ' secs Frame = ' + str(frameNumber) + ' saved successfully')\n","\n","  else:\n","\n","    break\n","\n","  # every time you hit the last frame increase the time by 1 and reset the frames to 0\n","  if frameNumber == fps:\n","\n","    timeStamp += 1\n","    frameNumber = 0\n","\n","print(\"Frame successfully Processed.\")\n","inputVideo.release()"]},{"cell_type":"markdown","metadata":{"id":"yOpdsUhvBXZm"},"source":["#### Inference code for VQA (Visual Question Answering)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SCpMnzRHY8ta"},"outputs":[],"source":["from models.blip_vqa import blip_vqa\n","from models.blip import blip_decoder\n","from PIL import Image\n","import requests\n","import torch\n","from torchvision import transforms\n","from torchvision.transforms.functional import InterpolationMode\n","import cv2\n","import time\n","\n","# question to be Asked (Text Prompt)\n","question = 'What is the facial expression of the person?'\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","currentTime = time.strftime(\"%H_%M_%S\")\n","\n","#path to input Video File\n","pathToInputVideoFile = '/content/drive/My Drive/My_Software_Projects/Input_Video/InputVideo2.mp4'\n","\n","#path to the output Folder\n","pathToOutputFramesFolder = '/content/drive/My Drive/My_Software_Projects/Output/Output_VQA_' + str(currentTime) + '/'\n","makeDirectory(pathToOutputFramesFolder)\n","\n","# path to the output text File\n","pathToOutputTxtFile = '/content/drive/My Drive/My_Software_Projects/Output_File/CaptionTextFile_' + str(currentTime) + '.txt'\n","\n","imageSize = 480\n","# image = load_image(image_size=image_size, device=device)\n","\n","modelUrl = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n","\n","print(\"Loading the Pre-Trained Model...\")\n","\n","model = blip_vqa(pretrained=modelUrl, image_size=imageSize, vit='base')\n","model.eval()\n","model = model.to(device)\n","print(\"Model Successfully Loaded.\")\n","\n","print(\"Loading Input Video File.....\")\n","inputVideo = cv2.VideoCapture(pathToInputVideoFile)\n","print(\"Successfully Loaded input File\")\n","\n","\n","# Calculate the Frames per second (FPS)\n","print(\"Calculating Frames Per Second...\")\n","fps = round(inputVideo.get(cv2.CAP_PROP_FPS))\n","print('Fps = ' + str(fps))\n","\n","frameNumber = 0\n","timeStamp = 0\n","\n","print('Processing Frames...')\n","\n","\n","while True:\n","  # Processing Frames\n","  success, imageFrame = inputVideo.read()\n","\n","  if success:\n","    # increase the frame by 1\n","    frameNumber += 1\n","\n","    image = loadImage(imageSize=imageSize, device=device, rawImage=imageFrame)\n","\n","    with torch.no_grad():\n","\n","      answer = model(image, question, train=False, inference='generate')\n","      print('answer: '+answer[0])\n","\n","\n","      outputFrameFilePath = pathToOutputFramesFolder + str(answer[0]) + str(timeStamp) + str(frameNumber) + '.jpg'\n","\n","      # write the frame\n","      cv2.imwrite(outputFrameFilePath, imageFrame)\n","\n","      # write the captions\n","      writingText = \"TimeStamp = \" + str(timeStamp) +  \" Frame = \" + str(frameNumber) +  \" Emotion : \" + str(answer[0])\n","      # utputTextFile.write(writingText)\n","\n","      with open(pathToOutputTxtFile, 'a') as testwritefile:\n","        testwritefile.write(writingText + '\\n')\n","\n","      # outputTextFile.write('\\n')\n","\n","      print('Time = ' + str(timeStamp) + ' secs Frame = ' + str(frameNumber) + ' saved successfully')\n","\n","  else:\n","\n","    break\n","\n","  # every time you hit the last frame increase the time by 1 and reset the frames to 0\n","  if frameNumber == fps:\n","\n","    timeStamp += 1\n","    frameNumber = 0\n","\n","print(\"Frame successfully Processed.\")\n","inputVideo.release()\n","\n"]},{"cell_type":"markdown","source":["#### Code to generate Dataset containing Timestamp, Image/FrameId, FileName,  CaptionedText, QAResult, AudiotoText"],"metadata":{"id":"g3TOsRDohr7n"}},{"cell_type":"code","source":["from models.blip_vqa import blip_vqa\n","from models.blip import blip_decoder\n","from PIL import Image\n","import requests\n","import torch\n","from torchvision import transforms\n","from torchvision.transforms.functional import InterpolationMode\n","import cv2\n","import time\n","import csv\n","\n","# question to be Asked (Text Prompt)\n","question = 'What is the facial expression of the person?'\n","\n","# csv object to be written to the csv File\n","csvWrite = ['Timestamp', 'FrameID', 'FileName', 'CaptionedText', 'QAResult', 'AudioToText']\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","currentTime = time.strftime(\"%H_%M_%S\")\n","\n","#path to input Video File\n","pathToInputVideoFile = '/content/drive/My Drive/My_Software_Projects/Input_Video/InputVideo2.mp4'\n","\n","#path to the output Folder\n","pathToOutputFramesFolder = '/content/drive/My Drive/My_Software_Projects/Output/Output_Dataset_' + str(currentTime) + '/'\n","makeDirectory(pathToOutputFramesFolder)\n","\n","# path to the output text File\n","pathToOutputTxtFile = '/content/drive/My Drive/My_Software_Projects/Output_File/CaptionTextFile_' + str(currentTime) + '.txt'\n","pathToOutputCSVFile = '/content/drive/My Drive/My_Software_Projects/Output_File/CaptionCSVFile_' + str(currentTime) + '.csv'\n","\n","csvWriteRow(pathToCSVFile=pathToOutputCSVFile, rowData=csvWrite)\n","\n","imageSizeVQA = 480\n","imageSizeIC = 384\n","# image = load_image(image_size=image_size, device=device)\n","\n","\n","# Url to the model file\n","modelUrlVQA = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n","modelUrlIC = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n","\n","# Video to text conversion\n","audioToTextList = videoToText(pathToVideo = pathToInputVideoFile)\n","print (\"Conversion from speech to text successful.\")\n","\n","\n","print(\"Loading the Pre-Trained Image captioning Model...\")\n","modelIC = blip_decoder(pretrained=modelUrlIC, image_size=imageSizeIC, vit='base')\n","modelIC.eval()\n","modelIC = modelIC.to('cpu')\n","print(\"Image cpationing Model Successfully Loaded.\")\n","\n","print(\"Loading the Pre-Trained VQA Model...\")\n","modelVQA = blip_vqa(pretrained=modelUrlVQA, image_size=imageSizeVQA, vit='base')\n","modelVQA.eval()\n","modelVQA = modelVQA.to('cuda')\n","print(\"VQA Model Successfully Loaded.\")\n","\n","print(\"Loading Input Video File.....\")\n","inputVideo = cv2.VideoCapture(pathToInputVideoFile)\n","print(\"Successfully Loaded input File\")\n","\n","\n","# Calculate the Frames per second (FPS)\n","print(\"Calculating Frames Per Second...\")\n","fps = round(inputVideo.get(cv2.CAP_PROP_FPS))\n","print('Fps = ' + str(fps))\n","\n","frameNumber = 0\n","timeStamp = 0\n","\n","print('Processing Frames...')\n","\n","\n","while True:\n","  # Processing Frames\n","  success, imageFrame = inputVideo.read()\n","\n","  if success:\n","    # increase the frame by 1\n","    frameNumber += 1\n","\n","    imageVQA = loadImage(imageSize=imageSizeVQA, device='cuda', rawImage=imageFrame)\n","    imageIC = loadImage(imageSize=imageSizeIC, device='cpu', rawImage=imageFrame)\n","\n","    with torch.no_grad():\n","\n","      answer = modelVQA(imageVQA, question, train=False, inference='generate')\n","      caption = modelIC.generate(imageIC, sample=True, top_p=0.9, max_length=20, min_length=5)\n","      print('answer: '+answer[0])\n","\n","\n","      #outputFrameFilePath = pathToOutputFramesFolder + str(answer[0]) + str(timeStamp) + str(frameNumber) + '.jpg'\n","      outputFrameFileName = 'time_' + str(timeStamp) + 'frame_' + str(frameNumber) + '.jpg'\n","      outputFrameFilePath = pathToOutputFramesFolder + outputFrameFileName\n","\n","      # write the frame\n","      cv2.imwrite(outputFrameFilePath, imageFrame)\n","\n","      # write the captions\n","      # writingText = \"TimeStamp = \" + str(timeStamp) +  \" Frame = \" + str(frameNumber) + \" Emotion : \" + str(answer[0])\n","      # utputTextFile.write(writingText)\n","\n","      # append a row to csvFile object\n","      csvWrite = [str(timeStamp), str(frameNumber), outputFrameFileName, str(caption[0]), str(answer[0]), str(audioToTextList[timeStamp])]\n","      csvWriteRow(pathToCSVFile=pathToOutputCSVFile, rowData=csvWrite)\n","\n","      #with open(pathToOutputTxtFile, 'a') as testwritefile:\n","        #testwritefile.write(writingText + '\\n')\n","\n","      # outputTextFile.write('\\n')\n","\n","      print('Time = ' + str(timeStamp) + ' secs Frame = ' + str(frameNumber) + ' saved successfully')\n","\n","  else:\n","\n","    break\n","\n","  # every time you hit the last frame increase the time by 1 and reset the frames to 0\n","  if frameNumber == fps:\n","\n","    timeStamp += 1\n","    frameNumber = 0\n","\n","print(\"Frame successfully Processed.\")\n","inputVideo.release()\n","\n","# write the csv File\n","#with open(pathToOutputCSVFile, 'w', newline='') as testWriteCSV:\n","#  csvWriter = csv.writer(testWriteCSV)\n","#  csvWriter.writerows(csvWrite)"],"metadata":{"id":"1A123zdohsP2"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["RI9etoGooW7c","lFQpveR-Fd8w","yOpdsUhvBXZm"],"authorship_tag":"ABX9TyNIOZV5qMjGz0J+BCpLrsJs"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}